{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wikipedia.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tirw9HnrTuau"
      },
      "source": [
        "# 1.0 Case Study: constructing a network of wikipedia pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryNuGuF5aK6t"
      },
      "source": [
        "## 1.1 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4U4cYABYxsK"
      },
      "source": [
        "So far we have learned two ways of constructing a complex network:\n",
        "- from a CSV file \n",
        "- manually\n",
        "\n",
        "What is hard for small networks may be impossible for medium-to-large scale networks; it may be impossible even for small networks if you must repeat the analysis many\n",
        "times. The case study in this lesson shows you **how to construct a large\n",
        "network in an easy way**:  \n",
        "- automatically collecting node and edge data from the Internet.\n",
        "\n",
        "The other goal of this study (aside from mastering new network construction\n",
        "techniques) is quite pragmatic. \n",
        "- Wouldn’t you want to know where the **complex\n",
        "network analysis** fits in the context of other subjects and disciplines? \n",
        "\n",
        "An answer to this question is near at hand: [on Wikipedia](https://en.wikipedia.org/wiki/Complex_network).\n",
        "\n",
        "\n",
        "Let’s start with the Wikipedia page about complex networks—the seed page.\n",
        "(Unfortunately, there is no page on complex network analysis itself.) The page\n",
        "body has external links and links to other Wikipedia pages. Those other pages\n",
        "presumably are somewhat related to complex networks, or else why would\n",
        "the Wikipedia editors provide them?\n",
        "To build a network out of the seed page and other relevant pages, **let’s treat\n",
        "the pages (and the respective Wikipedia subjects) as the network nodes and\n",
        "the links between the pages as the network edges**. You will use snowball\n",
        "sampling (a breadth-first search or [BFS algorithm](https://en.wikipedia.org/wiki/Breadth-first_search)) to discover all the nodes and edges of interest.\n",
        "\n",
        "<img width=\"200\" src=\"https://drive.google.com/uc?export=view&id=1t9ZXB5Q-wRt_qWbf8eU_1RiTwVGaStHU\">\n",
        "\n",
        "As a result, you will have a network of all pages related to complex networks\n",
        "and hopefully, you will make some conclusions about it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnNVv00Aalcx"
      },
      "source": [
        "## 1.2 Get the Data, Build the Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DofZOgJa9Nf"
      },
      "source": [
        "The first half of the project script consists of:\n",
        "\n",
        "1. the initialization prologue\n",
        "2. a heavy-duty loop that retrieves the Wikipedia pages and simultaneously builds the network of nodes and edges.\n",
        "\n",
        "Let’s first import all necessary modules. We will need:\n",
        "\n",
        "- the module **wikipedia** for fetching and exploring Wikipedia pages\n",
        "- the operator **itemgetter** for sorting a list of tuples\n",
        "- and, naturally, **networkx** itself.\n",
        "\n",
        "To target the **snowballing process**, define the constant **SEED, the name of the\n",
        "starting page**. \n",
        "\n",
        "> As a side note, by changing the name of the seed page, you can\n",
        "apply this analysis to any other subject on Wikipedia.\n",
        "\n",
        "\n",
        "Last but not least, when you start the snowballing, you will eventually (and\n",
        "quite soon) bump into the pages describing ISBN and ISSN numbers, the\n",
        "arXiv, PubMed, and the like. Almost all other Wikipedia pages refer to one or\n",
        "more of those pages. This hyper-connectedness transforms any network into\n",
        "a collection of almost perfect gigantic stars, making all Wikipedia-based networks\n",
        "look similar. **To avoid the stardom syndrome**, treat the known “star”\n",
        "pages as stop words in information retrieval—in other words, ignore any links\n",
        "to them. \n",
        "\n",
        "Constructing the **black list of stop words**, STOPS, is a matter of trial\n",
        "and error. We put thirteen subjects on it; you may want to add more when you\n",
        "come across other “stars.” We also excluded pages whose names begin with\n",
        "**\"List of\"**, because they are simply lists of other subjects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyD6-e-34h5P"
      },
      "source": [
        "# always check the current version in github\n",
        "!pip install networkx==2.6.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQcOSXayXX1W"
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MINWDN4gTzAC"
      },
      "source": [
        "from operator import itemgetter\n",
        "import networkx as nx\n",
        "import wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVqdt1994G2J"
      },
      "source": [
        "nx.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPTw8l5iT7z9"
      },
      "source": [
        "#https://en.wikipedia.org/wiki/Complex_network\n",
        "SEED = \"Complex network\".title()\n",
        "STOPS = (\"International Standard Serial Number\",\n",
        "         \"International Standard Book Number\",\n",
        "         \"National Diet Library\",\n",
        "         \"International Standard Name Identifier\",\n",
        "         \"International Standard Book Number (Identifier)\",\n",
        "         \"Pubmed Identifier\", \n",
        "         \"Pubmed Central\",\n",
        "         \"Digital Object Identifier\", \n",
        "         \"Arxiv\",\n",
        "         \"Proc Natl Acad Sci Usa\", \n",
        "         \"Bibcode\",\n",
        "         \"Library Of Congress Control Number\", \n",
        "         \"Jstor\",\n",
        "         \"Doi (Identifier)\",\n",
        "         \"Isbn (Identifier)\",\n",
        "         \"Pmid (Identifier)\",\n",
        "         \"Arxiv (Identifier)\",\n",
        "         \"Bibcode (Identifier)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChlfDfm7YQNY"
      },
      "source": [
        "The next code fragment deals with setting up the **snowballing process**. A\n",
        "breadth-first search, or BFS (sometimes known to computer programmers\n",
        "as a snowballing algorithm), must remember which **pages have been already\n",
        "processed** and which have been **discovered but not yet processed**. \n",
        "- the former are stored in the set **done_set**; \n",
        "- the latter, in the list **todo_lst** and **todo_set**. \n",
        "\n",
        "You need two data structures for the unprocessed pages because you want to\n",
        "know whether a page has been already recorded (an unordered lookup, *todo_set*) and\n",
        "which page is the next to be processed (an ordered lookup, *todo_lst*). \n",
        "\n",
        "\n",
        "Snowballing an extensive network—and Wikipedia with 6,196,713 articles in\n",
        "the English segment alone can produce a huge network!—takes considerable\n",
        "time.\n",
        "\n",
        "<img width=\"300\" src=\"https://drive.google.com/uc?export=view&id=1cOpkuhQEHJ3UxQOP3hoCd3KzSlqyf1AG\">\n",
        "\n",
        "Suppose you start with one seed node, and let’s say it has $N\\approx 100$ neighbors.\n",
        "Each of them has $N$ neighbors, too, to the total of $\\approx N+N×N$ nodes. The\n",
        "third round of discovery adds $\\approx N×N×N$ more nodes. The time to shave each\n",
        "next layer of nodes grows exponentially. For this exercise, let’s process only\n",
        "the seed node itself and its immediate neighbors (layers 0 and 1). Processing\n",
        "layer 2 is still feasible, but layer 3 requires $N×N×N×N\\approx 10^8$ page downloads. To keep track of the distance from the currently processed node to the seed, store both the layer to which a node belongs and the node name together as a tuple on the **todo_lst** list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy1gJQxwYdNi"
      },
      "source": [
        "todo_lst = [(0, SEED)] # The SEED is in the layer 0\n",
        "todo_set = set(SEED) # The SEED itself\n",
        "done_set = set() # Nothing is done yet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlcRZ49jYjSr"
      },
      "source": [
        "**The output of the exercise is a NetworkX graph**. The next fragment will create\n",
        "an empty directed graph that will later absorb discovered nodes and edges.\n",
        "\n",
        "> We choose a **directed graph** because the edges that represent HTML links\n",
        "are naturally directed: a link from page A to page B does not imply a reciprocal\n",
        "link. \n",
        "\n",
        "The same fragment primes the algorithm by extracting the first “to-do” item\n",
        "(both its layer and page name) from the namesake list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z04Y-it9g5OR"
      },
      "source": [
        "g = nx.DiGraph()\n",
        "layer, page = todo_lst[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCe-Cq_4hGPH"
      },
      "source": [
        "It may take a fraction of a second to execute the first five lines of the script.\n",
        "It may take the whole next year or longer to finish the next twenty lines\n",
        "because they contain the main collection/construction loop of the project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucGHcQ4-hP4V"
      },
      "source": [
        "%%time\n",
        "while layer < 2:\n",
        "  # Remove the name page of the current page from the todo_lst, \n",
        "  # and add it to the set of processed pages. \n",
        "  # If the script encounters this page again, it will skip over it.\n",
        "  del todo_lst[0]\n",
        "  done_set.add(page)\n",
        "  \n",
        "  # Show progress\n",
        "  print(layer, page) \n",
        "  \n",
        "  # Attempt to download the selected page.\n",
        "  try:\n",
        "    wiki = wikipedia.page(page)\n",
        "  except:\n",
        "    layer, page = todo_lst[0]\n",
        "    print(\"Could not load\", page)\n",
        "    continue\n",
        "  \n",
        "  for link in wiki.links:\n",
        "    link = link.title()\n",
        "    if link not in STOPS and not link.startswith(\"List Of\"):\n",
        "      if link not in todo_set and link not in done_set:\n",
        "        todo_lst.append((layer + 1, link))\n",
        "        todo_set.add(link)\n",
        "      g.add_edge(page, link)\n",
        "  layer, page = todo_lst[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrtewM_hPqJa"
      },
      "source": [
        "print(\"{} nodes, {} edges\".format(len(g), nx.number_of_edges(g)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMX9A1-Nh-8B"
      },
      "source": [
        "The network of interest is now in the variable g. But it is “dirty”: inaccurate, incomplete, and erroneous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS911QoZMzb7"
      },
      "source": [
        "## 1.3 Eliminate Duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJ_c2SOHNa7O"
      },
      "source": [
        "Many Wikipedia pages exist under two or more names. For example, there\n",
        "are pages about **Complex Network** and **Complex Networks**. The latter redirects\n",
        "to the former, but NetworkX does not know about the redirection.\n",
        "\n",
        "Accurately merging all duplicate nodes involves **natural language processing\n",
        "(NLP) tools that are outside of the scope of this course**. It may suffice to join\n",
        "only those nodes that differ by the presence/absence of the letter s at the end\n",
        "or a hyphen in the middle.\n",
        "\n",
        "Start removing self-loops (pages referring to themselves). The loops don’t change\n",
        "the network properties but affect the correctness of duplicate node elimination.\n",
        "Now, you need a list of at least some duplicate nodes. You can build it by\n",
        "looking at each node in g and checking if a node with the same name, but\n",
        "with an s at the end, is also in g. \n",
        "\n",
        "Pass each pair of duplicated node names to the function **nx.contracted_nodes(g,u,v)** that merges node v into node u in the graph g. The function reassigns all edges previously incident to v, to u. If you don’t pass the option **self_loops=False**, the function converts an edge from v to u (if any) to a self-loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VkbRjFcBQaX"
      },
      "source": [
        "# make a copy of raw graph\n",
        "original = g.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFrNIuluN1__"
      },
      "source": [
        "# remove self loops\n",
        "g.remove_edges_from(nx.selfloop_edges(g))\n",
        "\n",
        "# identify duplicates like that: 'network' and 'networks'\n",
        "duplicates = [(node, node + \"s\") \n",
        "              for node in g if node + \"s\" in g\n",
        "             ]\n",
        "\n",
        "for dup in duplicates:\n",
        "  # *dup is a technique named 'unpacking'\n",
        "  g = nx.contracted_nodes(g, *dup, self_loops=False)\n",
        "\n",
        "print(duplicates)\n",
        "\n",
        "duplicates = [(x, y) for x, y in \n",
        "              [(node, node.replace(\"-\", \" \")) for node in g]\n",
        "                if x != y and y in g]\n",
        "print(duplicates)\n",
        "\n",
        "for dup in duplicates:\n",
        "  g = nx.contracted_nodes(g, *dup, self_loops=False)\n",
        "\n",
        "# nx.contracted creates a new node/edge attribute called contraction\n",
        "# the value of the attribute is a dictionary, but GraphML\n",
        "# does not support dictionary attributes\n",
        "nx.set_node_attributes(g, 0,\"contraction\")\n",
        "nx.set_edge_attributes(g, 0,\"contraction\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bqe2ruDmPcNH"
      },
      "source": [
        "print(\"{} nodes, {} edges\".format(len(g), nx.number_of_edges(g)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TulPj3ZQQAS8"
      },
      "source": [
        "## 1.4 Truncate the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGjVZK35URcH"
      },
      "source": [
        "Why did you go through all those Wikipedia troubles? First, to construct a\n",
        "network of subjects related to complex networks—and here it is. Second, to\n",
        "find other significant topics related to complex networks. **But what is the\n",
        "measure of significance?**\n",
        "\n",
        "\n",
        "You will discover a variety of network measures latter in this course. For now, let’s concentrate on a **node indegree**—the number of edges directed into the node. (In the same spirit, the number of edges directed out of the node is called **outdegree**.) The indegree of a node equals the number of HTML links pointing to the respective page. **If a page has a lot of links to it, the topic of the page must be significant.**\n",
        "\n",
        "\n",
        "The choice of indegree as a yardstick of significance incidentally makes it\n",
        "possible to shrink the graph size by almost 75 percent. The extracted graph\n",
        "has 13,401 nodes and 24104 edges—an average of 1.79 edges per node. **Most\n",
        "of the nodes have only one connection**. (Interestingly, there are no isolated\n",
        "nodes with no connection in the graph. Even if they exist, you will not find\n",
        "them because of the way snowballing works.) **You can remove all nodes with\n",
        "only one incident edge to make the network more compact and less hairy\n",
        "without hurting the final results**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaHxaOwMLW9B"
      },
      "source": [
        "plt.style.use(\"default\")\n",
        "# degree sequence\n",
        "degree_sequence = sorted([d for n, d in g.degree()], reverse=True)  \n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(8,4))\n",
        "\n",
        "# all_data has information about degree_sequence and the width of each bin\n",
        "ax[0].hist(degree_sequence)\n",
        "ax[1].hist(degree_sequence,bins=[1,2,3,4,5,6,7,8,9,10])\n",
        "\n",
        "ax[0].set_title(\"Degree Histogram\")\n",
        "ax[0].set_ylabel(\"Count\")\n",
        "ax[0].set_xlabel(\"Degree\")\n",
        "ax[0].set_ylim(0,15000)\n",
        "\n",
        "ax[1].set_title(\"Degree Histogram - Zoom\")\n",
        "ax[1].set_ylabel(\"Count\")\n",
        "ax[1].set_xlabel(\"Degree\")\n",
        "ax[1].set_xlim(0,10)\n",
        "ax[1].set_ylim(0,15000)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br8-5IGIUbVT"
      },
      "source": [
        "# filter nodes with degree greater than or equal to 2\n",
        "core = [node for node, deg in dict(g.degree()).items() if deg >= 2]\n",
        "\n",
        "# select a subgraph with 'core' nodes\n",
        "gsub = nx.subgraph(g, core)\n",
        "\n",
        "print(\"{} nodes, {} edges\".format(len(gsub), nx.number_of_edges(gsub)))\n",
        "\n",
        "nx.write_graphml(gsub, \"cna.graphml\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD2AghG1eXMB"
      },
      "source": [
        "print(\"Nodes removed: {:.2f}%\".format(100*(1 - 3195/13899)))\n",
        "print(\"Edges removed: {:.2f}%\".format(100*(1 - 13899/24105)))\n",
        "print(\"Edges per nodes: {:.2f}\".format(13899/3195))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSELbSFCX__7"
      },
      "source": [
        "As you can see, the following code fragment safely removes 77 percent of the\n",
        "nodes and 42 percent of the edges, raising the average number of edges per\n",
        "node to 4.35."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zRjfGLtZ2qB"
      },
      "source": [
        "## 1.5 Explore the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUMyWlK7jqkX"
      },
      "source": [
        "The following figure is a [Gephi](https://gephi.org/) rendering of **gsub**. Node\n",
        "and label font sizes represent the indegrees (a filter was added in gephi just to show only node with indegree greater than 50). The most in-connected, most significant\n",
        "nodes are in the upper-left corner of the network. What are they?\n",
        "\n",
        "<img width=\"600\" src=\"https://drive.google.com/uc?export=view&id=1Cemnpe6yBttyZ8Vw2i9ifMiWx9psIV7C\">\n",
        "\n",
        "\n",
        "The last code fragment of the exercise efficiently calculates the answer by\n",
        "calling the method **gsub.in_degree()**. The method (and its sister method **gsub.out_degree()**)\n",
        "are very similar to **gsub.degree()** except that they report different edge counts in the form of objects InDegreeView and OutDegreeView, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwlRxZ2kmDL-"
      },
      "source": [
        "top_indegree = sorted(dict(gsub.in_degree()).items(),\n",
        "                      reverse=True, key=itemgetter(1))[:100]\n",
        "print(\"\\n\".join(map(lambda t: \"{} {}\".format(*reversed(t)), top_indegree)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PQT-sl0mulb"
      },
      "source": [
        "**Exercise**\n",
        "\n",
        "<img width=\"100\" src=\"https://drive.google.com/uc?export=view&id=1E8tR7B9YYUXsU_rddJAyq0FrM0MSelxZ\">\n",
        "\n",
        "\n",
        "- This section presented a complete complex network construction case study,\n",
        "starting from the raw data in the form of HTML pages, all the way to an analyzable\n",
        "annotated network graph and a simple exploratory exercise. This is\n",
        "a good foundation for more systematic complex network studies. Now is your turn, choice a topic of your interest and reproduce yourself a complet complex network case study. \n",
        "\n",
        "- Some examples (not limited to):\n",
        "\n",
        "  - https://en.wikipedia.org/wiki/Black_Lives_Matter\n",
        "  - https://en.wikipedia.org/wiki/COVID-19\n",
        "  - https://en.wikipedia.org/wiki/Harry_Potter\n",
        "  - https://en.wikipedia.org/wiki/Sherlock_Holmes\n",
        "  - https://en.wikipedia.org/wiki/Augusto_Severo_de_Albuquerque_Maranh%C3%A3o\n",
        "  - https://en.wikipedia.org/wiki/Brazil\n",
        "  - https://en.wikipedia.org/wiki/Bitcoin\n",
        "  - https://en.wikipedia.org/wiki/Federal_University_of_Rio_Grande_do_Norte\n",
        "  - https://en.wikipedia.org/wiki/Petrobras\n",
        "  - https://en.wikipedia.org/wiki/List_of_presidents_of_Brazil\n",
        "- Explore some metrics studied in Unit 01 to highlight insights in your analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzoPkyYOnrud"
      },
      "source": [
        "# put your results here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}